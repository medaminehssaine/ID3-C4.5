# ğŸŒ³ ID3 & C4.5 Decision Trees

> Professional implementations of Quinlan's classic decision tree algorithms for the KDD course.

<div align="center">

| Algorithm | Year | Criterion | Continuous | Pruning |
|:---------:|:----:|:---------:|:----------:|:-------:|
| **ID3** | 1986 | Info Gain | âœ— | âœ— |
| **C4.5** | 1993 | Gain Ratio | âœ“ | âœ“ |

</div>

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/decision_trees/      # unified package
â”‚   â”œâ”€â”€ id3/                 # ID3 algorithm
â”‚   â”‚   â”œâ”€â”€ core/            # entropy, node, tree
â”‚   â”‚   â”œâ”€â”€ data/            # sample datasets
â”‚   â”‚   â”œâ”€â”€ utils/           # validation, visualization
â”‚   â”‚   â””â”€â”€ comparison/      # sklearn benchmarks
â”‚   â””â”€â”€ c45/                 # C4.5 algorithm
â”‚       â”œâ”€â”€ core/            # gain_ratio, pruning
â”‚       â”œâ”€â”€ data/            # continuous datasets
â”‚       â””â”€â”€ utils/           # visualization
â”œâ”€â”€ tests/                   # unit tests (26 total)
â”œâ”€â”€ examples/                # demo scripts
â”œâ”€â”€ outputs/                 # generated .dot files
â””â”€â”€ pyproject.toml           # pip installable
```

---

## ğŸš€ Quick Start

```python
# === ID3: categorical features ===
from decision_trees.id3 import ID3Classifier
from decision_trees.id3.data import load_play_tennis

X, y, names = load_play_tennis()
clf = ID3Classifier()
clf.fit(X, y, names)
print(clf.predict_one(("sunny", "cool", "normal", "weak")))
# â†’ "yes"


# === C4.5: continuous + categorical ===
from decision_trees.c45 import C45Classifier
from decision_trees.c45.data import load_iris

X, y, names = load_iris()
clf = C45Classifier()
clf.fit(X, y, names)
# auto-detects continuous features and finds thresholds
```

---

## ğŸ§ª Testing

```bash
python tests/test_id3.py   # 15 tests âœ“
python tests/test_c45.py   # 11 tests âœ“
```

---

## ğŸ¬ Demos

Beautiful terminal demos with colors, progress bars, and educational explanations.

```bash
python examples/demo_id3.py   # entropy, training, prediction, CV
python examples/demo_c45.py   # gain ratio, thresholds, pruning
```

**Demo highlights:**
- ğŸ“Š Visual entropy/gain calculations
- ğŸŒ³ Tree visualization in console
- ğŸ“ˆ Accuracy bars and metrics
- ğŸ”„ ID3 vs C4.5 comparison

---

## ğŸ“š Algorithm Details

### ID3 (Iterative Dichotomiser 3)

```
function ID3(D, features):
    if all samples same class â†’ return leaf
    if no features left â†’ return majority class
    
    best = argmax(features, key=InformationGain)
    node = new Node(best)
    
    for each value v of best:
        subset = samples where feature[best] = v
        node.children[v] = ID3(subset, features - {best})
    
    return node
```

**Key formula:**
```
H(S) = -Î£ p(c) Ã— logâ‚‚(p(c))        # entropy
IG(S, A) = H(S) - Î£ (|Sáµ¥|/|S|) Ã— H(Sáµ¥)   # info gain
```

---

### C4.5 Improvements

| Feature | How it works |
|---------|--------------|
| **Gain Ratio** | `GR = IG / SplitInfo` â€” penalizes high-cardinality |
| **Continuous** | Binary splits at optimal thresholds |
| **Missing** | Distributes samples proportionally |
| **Pruning** | Reduced error pruning on validation set |

---

## ğŸ”§ Installation

```bash
# run directly (no install needed)
python examples/demo_id3.py

# or install as package
pip install -e .

# then import anywhere
from decision_trees import ID3Classifier, C45Classifier
```

---

## ğŸ“Š Sample Output

**ID3 Tree (Play Tennis):**
```
[outlook?]
â”œâ”€â”€ sunny [humidity?]
â”‚   â”œâ”€â”€ high â†’ [no]
â”‚   â””â”€â”€ normal â†’ [yes]
â”œâ”€â”€ overcast â†’ [yes]
â””â”€â”€ rain [wind?]
    â”œâ”€â”€ weak â†’ [yes]
    â””â”€â”€ strong â†’ [no]
```

**C4.5 Tree (Iris):**
```
[petal_length <= 2.50?]
    yes: â†’ [setosa]
    no:  [petal_width <= 1.65?]
        yes: â†’ [versicolor]
        no:  â†’ [virginica]
```

---

## ğŸ‘¥ Team

KDD Course Project â€” Decision Tree Algorithms

---

<div align="center">
<sub>Built with â¤ï¸ for learning machine learning fundamentals</sub>
</div>
