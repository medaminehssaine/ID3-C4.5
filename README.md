# ğŸŒ³ ID3 & C4.5 Decision Trees

> Professional implementations of Quinlan's classic decision tree algorithms for the KDD course.
> Built from scratch with strict adherence to the original research papers.

<div align="center">

| Algorithm | Year | Criterion | Continuous | Missing Values | Pruning |
|:---------:|:----:|:---------:|:----------:|:--------------:|:-------:|
| **ID3** | 1986 | Info Gain | âœ— | âœ— | âœ— |
| **C4.5** | 1993 | Gain Ratio | âœ“ | âœ“ | âœ“ |

</div>

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/decision_trees/      # unified package
â”‚   â”œâ”€â”€ id3/                 # ID3 algorithm
â”‚   â”‚   â”œâ”€â”€ core/            # entropy, node, tree
â”‚   â”‚   â”œâ”€â”€ data/            # sample datasets
â”‚   â”‚   â”œâ”€â”€ utils/           # validation, visualization
â”‚   â”‚   â””â”€â”€ comparison/      # sklearn benchmarks
â”‚   â””â”€â”€ c45/                 # C4.5 algorithm
â”‚       â”œâ”€â”€ core/            # gain_ratio, pruning
â”‚       â”œâ”€â”€ data/            # continuous datasets
â”‚       â””â”€â”€ utils/           # visualization
â”œâ”€â”€ tests/                   # unit tests (40+ tests)
â”œâ”€â”€ examples/                # demo scripts
â”œâ”€â”€ outputs/                 # generated .dot files
â”œâ”€â”€ REPORT_STRUCTURE.md      # theory-to-code mapping
â””â”€â”€ pyproject.toml           # pip installable
```

---

## ğŸ”§ Installation

```bash
# Clone and navigate
cd "ID3 & C4.5"

# Install as editable package
pip install -e .

# Optional: install test and comparison dependencies
pip install -e ".[dev,compare]"
```

---

## ğŸš€ Quick Start

```python
# === ID3: categorical features ===
from decision_trees.id3 import ID3Classifier
from decision_trees.id3.data import load_play_tennis

X, y, names = load_play_tennis()
clf = ID3Classifier()
clf.fit(X, y, names)
print(clf.predict_one(("sunny", "cool", "normal", "weak")))
# â†’ "yes"


# === C4.5: continuous + categorical ===
from decision_trees.c45 import C45Classifier
from decision_trees.c45.data import load_iris

X, y, names = load_iris()
clf = C45Classifier()
clf.fit(X, y, names)
print(clf.feature_types_)  # ['continuous', 'continuous', ...]
# Auto-detects continuous features and finds thresholds
```

---

## ğŸ“ Mathematical Foundations

### Entropy (Shannon Entropy)

The foundation of both ID3 and C4.5. Measures uncertainty in a dataset.

```
H(S) = -Î£áµ¢ p(cáµ¢) Ã— logâ‚‚(p(cáµ¢))
```

**Properties:**
- `H(S) = 0` for pure sets (all same class)
- `H(S) = 1` for balanced binary (50/50 split)
- `H(S) = logâ‚‚(k)` for k equally distributed classes

---

### Information Gain (ID3 Criterion)

Measures reduction in entropy after splitting.

```
IG(S, A) = H(S) - Î£áµ¥ (|Sáµ¥|/|S|) Ã— H(Sáµ¥)
```

**Problem:** Biased toward high-cardinality features!

*Example:* A unique ID column always has maximum IG but provides no generalization.

---

### Gain Ratio (C4.5 Solution)

Normalizes Information Gain to reduce bias.

```
GR(S, A) = IG(S, A) / SI(S, A)

SI(S, A) = -Î£áµ¥ (|Sáµ¥|/|S|) Ã— logâ‚‚(|Sáµ¥|/|S|)
```

**Why it works:**
- Split Information (SI) is high for features with many values
- Dividing IG by SI penalizes high-cardinality features
- `GR â‰¤ IG` always holds (SI â‰¥ 1 for 2+ partitions)

**Mathematical Proof:**
```
SI(S,A) = -Î£áµ¥ páµ¥ Ã— logâ‚‚(páµ¥)  where páµ¥ = |Sáµ¥|/|S|

For n equally sized partitions: SI = logâ‚‚(n) â‰¥ 1 when n â‰¥ 2
Therefore: GR = IG/SI â‰¤ IG
```

---

### Continuous Attribute Handling

C4.5 finds optimal thresholds for numeric features using binary splits.

**Algorithm:**
1. Sort values
2. Find midpoints where class changes
3. Evaluate GR for each candidate threshold
4. Choose threshold with maximum GR

**Key difference from ID3:** Continuous features can be reused in subtrees!

---

### Pessimistic Error Pruning

C4.5's default pruning method (no validation set required).

```
Pessimistic Error = (errors + 0.5) / N
```

Uses Wilson score interval for tighter bounds:
```
UCB = (f + zÂ²/2n + zÃ—âˆš(f(1-f)/n + zÂ²/4nÂ²)) / (1 + zÂ²/n)
```

---

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Or run individually
python tests/test_id3.py   # 21 tests
python tests/test_c45.py   # 23 tests
```

### Key Test Cases

| Test | Formula Verified |
|------|------------------|
| `test_entropy_classic` | H([9+, 5-]) â‰ˆ 0.9403 |
| `test_gain_ratio_less_than_ig` | GR â‰¤ IG always |
| `test_best_threshold` | Optimal threshold at class boundary |

---

## ğŸ¬ Demos

Beautiful terminal demos with colors, progress bars, and educational explanations.

```bash
python examples/demo_id3.py   # entropy, training, prediction, CV
python examples/demo_c45.py   # gain ratio, thresholds, pruning
```

**Demo highlights:**
- ğŸ“Š Visual entropy/gain calculations
- ğŸŒ³ Tree visualization in console
- ğŸ“ˆ Accuracy bars and metrics
- ğŸ”„ ID3 vs C4.5 comparison

---

## ğŸ“š References

1. **Quinlan, J.R. (1986).** *"Induction of Decision Trees"*, Machine Learning 1:81-106
   - Original ID3 algorithm

2. **Quinlan, J.R. (1993).** *"C4.5: Programs for Machine Learning"*, Morgan Kaufmann
   - Gain Ratio, continuous attributes, pruning

3. **Shannon, C.E. (1948).** *"A Mathematical Theory of Communication"*
   - Foundation of entropy concept

---

## ğŸ“Š Sample Output

**ID3 Tree (Play Tennis):**
```
[outlook?]
â”œâ”€â”€ sunny [humidity?]
â”‚   â”œâ”€â”€ high â†’ [no]
â”‚   â””â”€â”€ normal â†’ [yes]
â”œâ”€â”€ overcast â†’ [yes]
â””â”€â”€ rain [wind?]
    â”œâ”€â”€ weak â†’ [yes]
    â””â”€â”€ strong â†’ [no]
```

**C4.5 Tree (Iris):**
```
[petal_length <= 2.50?]
    yes: â†’ [setosa]
    no:  [petal_width <= 1.65?]
        yes: â†’ [versicolor]
        no:  â†’ [virginica]
```

---

## ğŸ‘¥ Team

KDD Course Project â€” ID3 & C4.5 Decision Trees

| Name | Role |
|------|------|
| Mohammed Amine Hssaine | Implementation |
| Ouissam Benalla | Implementation |
| Mohamed Taha El Younsi | Implementation |

---

<div align="center">
<sub>Built with â¤ï¸ for learning machine learning fundamentals</sub>
</div>
